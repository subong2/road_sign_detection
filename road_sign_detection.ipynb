{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = Path('./Datasets/images')\n",
    "anno_path = Path('./Datasets/annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filelist(root, file_type):\n",
    "    \"\"\"Returns a fully-qualified list of filenames under root directory\"\"\"\n",
    "    return [os.path.join(directory_path, f) for directory_path, directory_name, \n",
    "            files in os.walk(root) for f in files if f.endswith(file_type)]\n",
    "\n",
    "def generate_train_df (anno_path):\n",
    "    annotations = filelist(anno_path, '.xml')\n",
    "    anno_list = []\n",
    "    for anno_path in annotations:\n",
    "        root = ET.parse(anno_path).getroot()\n",
    "        anno = {}\n",
    "        anno['filename'] = Path(str(images_path) + '/'+ root.find(\"./filename\").text)\n",
    "        anno['bb_info'] = [int(root.find(\"./object/bndbox/xmin\").text),int(root.find(\"./object/bndbox/ymin\").text),int(root.find(\"./object/bndbox/xmax\").text),int(root.find(\"./object/bndbox/ymax\").text)]\n",
    "        anno['class'] = root.find(\"./object/name\").text\n",
    "        anno_list.append(anno)\n",
    "    return pd.DataFrame(anno_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = generate_train_df(anno_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>bb_info</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Datasets/images/road712.png</td>\n",
       "      <td>[98, 140, 139, 182]</td>\n",
       "      <td>speedlimit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Datasets/images/road706.png</td>\n",
       "      <td>[136, 92, 177, 135]</td>\n",
       "      <td>speedlimit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Datasets/images/road289.png</td>\n",
       "      <td>[61, 140, 146, 227]</td>\n",
       "      <td>stop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Datasets/images/road538.png</td>\n",
       "      <td>[115, 169, 149, 205]</td>\n",
       "      <td>speedlimit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Datasets/images/road510.png</td>\n",
       "      <td>[89, 201, 133, 245]</td>\n",
       "      <td>speedlimit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>Datasets/images/road535.png</td>\n",
       "      <td>[100, 254, 180, 334]</td>\n",
       "      <td>speedlimit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>Datasets/images/road284.png</td>\n",
       "      <td>[111, 133, 165, 187]</td>\n",
       "      <td>speedlimit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>Datasets/images/road290.png</td>\n",
       "      <td>[105, 157, 171, 224]</td>\n",
       "      <td>speedlimit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>Datasets/images/road723.png</td>\n",
       "      <td>[115, 185, 160, 230]</td>\n",
       "      <td>speedlimit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>Datasets/images/road737.png</td>\n",
       "      <td>[115, 151, 143, 179]</td>\n",
       "      <td>speedlimit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>877 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        filename               bb_info       class\n",
       "0    Datasets/images/road712.png   [98, 140, 139, 182]  speedlimit\n",
       "1    Datasets/images/road706.png   [136, 92, 177, 135]  speedlimit\n",
       "2    Datasets/images/road289.png   [61, 140, 146, 227]        stop\n",
       "3    Datasets/images/road538.png  [115, 169, 149, 205]  speedlimit\n",
       "4    Datasets/images/road510.png   [89, 201, 133, 245]  speedlimit\n",
       "..                           ...                   ...         ...\n",
       "872  Datasets/images/road535.png  [100, 254, 180, 334]  speedlimit\n",
       "873  Datasets/images/road284.png  [111, 133, 165, 187]  speedlimit\n",
       "874  Datasets/images/road290.png  [105, 157, 171, 224]  speedlimit\n",
       "875  Datasets/images/road723.png  [115, 185, 160, 230]  speedlimit\n",
       "876  Datasets/images/road737.png  [115, 151, 143, 179]  speedlimit\n",
       "\n",
       "[877 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encode target\n",
    "class_dict = {'speedlimit': 0, 'stop': 1, 'crosswalk': 2, 'trafficlight': 3}\n",
    "df_train['class'] = df_train['class'].apply(lambda x:  class_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.6, p=0.4),\n",
    "    A.Resize(height=300, width=447, p=1.0),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_transform = A.Compose([\n",
    "    A.Resize(height=300, width=447, p=1.0),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[['filename', 'bb_info']]\n",
    "Y = df_train['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class RoadDataset(Dataset):\n",
    "    def __init__(self, paths, bb, y, transforms=False):\n",
    "        self.transforms = transforms\n",
    "        self.paths = paths.values\n",
    "        self.bb = bb.values\n",
    "        self.y = y.values\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        y_class = self.y[idx]\n",
    "        bb_info = self.bb[idx]\n",
    "        \n",
    "        image = cv2.imread(str(path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        \n",
    "        if self.transforms:\n",
    "\n",
    "            tf_result = self.transforms(image=image , bboxes=[bb_info], class_labels=y_class)\n",
    "            x = tf_result['image']\n",
    "            y_bb = tf_result['bboxes'][0]\n",
    "            y_bb = torch.tensor(y_bb, dtype=torch.float32)\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        \n",
    "        return x, y_class, y_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = RoadDataset(X_train['filename'],X_train['bb_info'] ,y_train, transforms=transform)\n",
    "valid_ds = RoadDataset(X_val['filename'],X_val['bb_info'],y_val,transforms=y_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 11\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # COCO 에서 미리 학습된 인스턴스 분할 모델을 읽어옵니다\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    num_classes = num_classes\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_instance_segmentation(num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]/Users/subong/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  2%|▏         | 1/64 [01:30<1:34:37, 90.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_classifier': tensor(172.5298, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0., grad_fn=<DivBackward0>), 'loss_objectness': tensor(86.1844, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.4941, grad_fn=<DivBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 2/64 [02:48<1:26:06, 83.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_classifier': tensor(174.4635, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(4.1543, grad_fn=<DivBackward0>), 'loss_objectness': tensor(66.2407, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(3.1223, grad_fn=<DivBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▍         | 3/64 [04:05<1:21:28, 80.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_classifier': tensor(165.3902, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0., grad_fn=<DivBackward0>), 'loss_objectness': tensor(78.2968, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.4444, grad_fn=<DivBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▋         | 4/64 [05:25<1:20:04, 80.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_classifier': tensor(164.4387, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0., grad_fn=<DivBackward0>), 'loss_objectness': tensor(76.9515, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.3512, grad_fn=<DivBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "iterator = tqdm.tqdm(train_dl)\n",
    "\n",
    "for images, y_class, y_bb in iterator :\n",
    "    d = {}\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(len(images)):\n",
    "        d['boxes'] = y_bb[[i]]\n",
    "        d['labels'] = y_class[[i]]\n",
    "        targets.append(d)\n",
    "    output = model(images, targets)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
